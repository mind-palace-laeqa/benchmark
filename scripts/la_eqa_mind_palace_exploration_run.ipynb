{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import yaml\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Now you can import the module\n",
    "from contextual_long_term_reasoning.mind_palace_generation import SceneGraph, LoadingHabitatSceneGraph, LoadingMaderoSceneGraph, LoadingIsaacSceneGraph\n",
    "from contextual_long_term_reasoning.belief_manager import WorldModel, BeliefManager\n",
    "from contextual_long_term_reasoning.eqa_reasoning import EQAReasoning\n",
    "from contextual_long_term_reasoning.mind_palace_exploration import MindPalaceExploration\n",
    "from contextual_long_term_reasoning.eqa_evaluation import EQAEvaluation\n",
    "from contextual_long_term_reasoning.ram_interface import RecognizeAnything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "eqa_file_path = \"../la_eqa_benchmark/eqa-questions.json\"\n",
    "episode_name_path = \"../la_eqa_benchmark/scene-episode-names.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(eqa_file_path, \"r\") as file:\n",
    "    eqa_questions = json.load(file)\n",
    "\n",
    "with open(episode_name_path, \"r\") as file:\n",
    "    episode_names = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_question_sets = eqa_questions[0:1]\n",
    "selected_question_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_load_pkl = True # Load the place_nodes.pkl file\n",
    "b_run_RAM = not b_load_pkl  # Run RAM to recognize objects for the first time\n",
    "b_save_the_logs = False # Save the logs\n",
    "\n",
    "result_name = \"ours\"\n",
    "\n",
    "if not os.path.exists(\"../results/\" + result_name):\n",
    "    os.makedirs(\"../results/\" + result_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "if b_save_the_logs:\n",
    "    # Save the logs in the file\n",
    "    original_stdout = sys.stdout\n",
    "    # Name the file with the date, time\n",
    "    now = datetime.datetime.now()\n",
    "    f = open(\"../results/\" + result_name + \"/1_results_\" + now.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\", \"w\")\n",
    "    sys.stdout = f\n",
    "\n",
    "prev_scene_name = \"\"\n",
    "for question_set in selected_question_sets:\n",
    "    question = question_set[\"question\"]\n",
    "    GT_A_answer = question_set[\"answer\"]\n",
    "    GT_best_path = question_set[\"best_path\"]\n",
    "    question_type = question_set[\"category\"]\n",
    "    # GT_minimal_image_retrieval = question_set[\"relevant_images\"]\n",
    "    robot_start_place = question_set[\"start_place\"]\n",
    "    scene_name = question_set[\"scene_name\"]\n",
    "    question_id = question_set[\"question_id\"]\n",
    "    episode_name_list = episode_names[scene_name]\n",
    "    if \"hbt_\" in question_id:\n",
    "        dataset_type = \"habitat\"\n",
    "    elif \"isc_\" in question_id:\n",
    "        dataset_type = \"isaac\"\n",
    "\n",
    "    result_file_path = os.path.join(\"../results/\" + result_name, question_id + \".yaml\")\n",
    "\n",
    "    # If the result path is there, skip\n",
    "    if os.path.exists(result_file_path):\n",
    "        print(f\"Result file {result_file_path} already exists. Skip this question.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n ##### Question ID: {question_id} \\n ##### Question: {question} \\n ##### Type: {question_type}\")\n",
    "\n",
    "    \n",
    "    if prev_scene_name == scene_name:\n",
    "        print(\"Same scene as previous question. Skip loading the scene.\")\n",
    "    else:\n",
    "        # 1 Dataset definition\n",
    "        if dataset_type == \"habitat\":\n",
    "            # Dataset paths\n",
    "            state_dataset_dir = \"../la_eqa_data/state/hm3d-v0\"\n",
    "            frames_dataset_dir = \"../la_eqa_data/frames/hm3d-v0\"\n",
    "            caption_dataset_dir = \"../la_eqa_data/captions/hm3d-v0\"\n",
    "\n",
    "            temporal_scene_name_dict = {}\n",
    "            for episode_name in episode_name_list:\n",
    "                temporal_scene_name_dict[episode_name] = scene_name + \"_\" + str(episode_name_list.index(episode_name))\n",
    "\n",
    "            state_dataset_path = os.path.join(state_dataset_dir, scene_name)\n",
    "            frames_dataset_path = os.path.join(frames_dataset_dir, scene_name)\n",
    "        elif dataset_type == \"isaac\":\n",
    "            dataset_dir = '../la_eqa_dataset/isaac_warehouse'\n",
    "\n",
    "            temporal_scene_name_dict = {}\n",
    "            for episode_name in episode_name_list:\n",
    "                temporal_scene_name_dict[episode_name] = scene_name + \"_\" + str(episode_name_list.index(episode_name))\n",
    "\n",
    "            \n",
    "        # 2 Mind palace generation\n",
    "        mind_palace = {}\n",
    "\n",
    "        if b_run_RAM:\n",
    "            recognize_anything_model = RecognizeAnything()\n",
    "        else:\n",
    "            recognize_anything_model = None\n",
    "\n",
    "        if dataset_type == \"habitat\":\n",
    "            for time_id, sn in temporal_scene_name_dict.items():\n",
    "                print(f\"Loading scene: {sn}\")\n",
    "                hbt_scene_loader = LoadingHabitatSceneGraph(sn, frames_dataset_dir, state_dataset_dir, \n",
    "                                                            recognize_anything_model, caption_dataset_dir)\n",
    "                \n",
    "                place_nodes = hbt_scene_loader.load_place_nodes(b_run_RAM, b_load_pkl)\n",
    "                room_nodes = hbt_scene_loader.load_room_nodes(place_nodes)\n",
    "                scene_graph = SceneGraph(sn, state_dataset_dir, room_nodes=room_nodes, place_nodes=place_nodes)\n",
    "                mind_palace[time_id] = scene_graph\n",
    "\n",
    "                print(scene_graph.print_room_nodes())\n",
    "\n",
    "        elif dataset_type == \"isaac\":\n",
    "            for time_id, sn in temporal_scene_name_dict.items():\n",
    "                print(f\"Loading scene: {sn}\")\n",
    "                madero_scene_loader = LoadingIsaacSceneGraph(dataset_dir, sn, recognize_anything_model)\n",
    "                \n",
    "                place_nodes = madero_scene_loader.load_place_nodes(b_run_RAM, b_load_pkl)\n",
    "                room_nodes = madero_scene_loader.load_room_nodes(place_nodes)\n",
    "                scene_graph = SceneGraph(sn, os.path.join(dataset_dir, sn, 'state/'), room_nodes=room_nodes, place_nodes=place_nodes)\n",
    "                mind_palace[time_id] = scene_graph\n",
    "\n",
    "    # If we don't load old pkl, return\n",
    "    if not b_load_pkl:\n",
    "        break\n",
    "        \n",
    "    # 3 Main loop\n",
    "    # Main algorithm\n",
    "    # 0. Initialize belief\n",
    "    belief_manager = BeliefManager(question, mind_palace)\n",
    "    world_model = WorldModel(mind_palace)\n",
    "    robot_place = copy.deepcopy(robot_start_place)\n",
    "\n",
    "    # 1. EQA reasoning and 2. Mind Palace Exploration\n",
    "    eqa_reasoning = EQAReasoning()\n",
    "    mind_palace_exploration = MindPalaceExploration()\n",
    "\n",
    "    # Params\n",
    "    max_reasoning_iter = 2\n",
    "    max_episode_exploration_iter = 4\n",
    "    max_room_exploration_iter = 5\n",
    "    max_num_place_node_exploration = 10\n",
    "\n",
    "    # Stats\n",
    "    stats_total_images_retrieved = 0\n",
    "    stats_total_distance = 0\n",
    "\n",
    "    reasoning_iter = 1\n",
    "    while reasoning_iter <= max_reasoning_iter:\n",
    "        print(f\"\\n##### Reasoning Iteration: {reasoning_iter}\\n\")\n",
    "        # 1. EQA reasoning\n",
    "\n",
    "        # 1.a. (Answer check) : Given Q, Memory/Belief(all the images retrieved so far and textual insight from the images), \n",
    "        # do we have enough information to answer the question? \n",
    "        bool_ready_to_answer, explanation = eqa_reasoning.check_ready_to_answer(belief_manager)\n",
    "        belief_manager.S_EQA_reasoning_summary.append(\"Reasoning Iteration: \" + str(reasoning_iter) + \":\\n\" + explanation + \"\\n\")\n",
    "        if bool_ready_to_answer:\n",
    "            print(\"Ready to answer the question!\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "\n",
    "        # 1.b If (Answer check) EQA Reasoning is No, do (Object identification) -> target object y : \n",
    "        # Given Q, Memory/Belief(all the images retrieved so far and textual insight from the images), \n",
    "        # What object/conceptual things that I need to look for in the environment to help answer the question? (For our example, black wallet),\n",
    "        belief_manager.y_object_to_search, belief_manager.y_reasoning_to_search_object = eqa_reasoning.object_identification(belief_manager)\n",
    "        belief_manager.S_EQA_reasoning_summary.append(\"Reasoning Iteration: \" + str(reasoning_iter) + \"Object to search: \" + str(belief_manager.y_object_to_search) + \" \" + str(belief_manager.y_reasoning_to_search_object) + \"\\n\")  \n",
    "\n",
    "        # 2. Mind Palace Exploration\n",
    "        # Given Q, Memory/Belief(all the images retrieved so far and textual insight from the images), \n",
    "        # What action (explore(pX) or retrieve(TX, pX)) that I need to do to find the target object y\n",
    "        # \t2.a.1 level_3_episode_selection -> TX # max exploration iter\n",
    "        T_episode_to_explore = mind_palace_exploration.episodic_exploration.plan(belief_manager)\n",
    "        list_T_episode_to_explore, search_strategy, reasoning_on_search_strategy, reasoning = mind_palace_exploration.episodic_exploration.episodic_reasoning_v2(belief_manager)\n",
    "        belief_manager.S_EQA_reasoning_summary.append(\"Reasoning Iteration: \" + str(reasoning_iter) + \":\\n Episodic Search Strategy: \\n\" + str(search_strategy) + str(reasoning_on_search_strategy) + \"\\nEpisode to explore \" + str(list_T_episode_to_explore) + str(reasoning) + \"\\n\")\n",
    "        \n",
    "        episode_exploration_iter = 1\n",
    "\n",
    "\n",
    "        for T_episode_to_explore_seq in list_T_episode_to_explore:\n",
    "            T_episode_now = belief_manager.get_now_episode()\n",
    "\n",
    "            if episode_exploration_iter <= max_episode_exploration_iter:\n",
    "                print(f\"\\n### Episode Exploration Iteration: {episode_exploration_iter}\\n\")\n",
    "                print(\"Episode to explore: \", T_episode_to_explore)\n",
    "            else:\n",
    "                print(\"Max episode exploration iteration reached. We will stop the exploration.\")\n",
    "                break\n",
    "\n",
    "            if stats_total_images_retrieved > 100:\n",
    "                print(\"Max number of images retrieved reached. We will stop the exploration.\")\n",
    "                break\n",
    "\n",
    "            bool_y_object_found = False\n",
    "            # 2.a (Object search) MindPalaceExploration -> action “a”: \n",
    "\n",
    "            room_exploration_iter = 1\n",
    "            \n",
    "            belief_manager.reset_room_and_place_exploration_memory()\n",
    "            while room_exploration_iter <= max_room_exploration_iter:\n",
    "                print(f\"\\n## Episode Exploration Iteration: {episode_exploration_iter}, Room Exploration Iteration: {room_exploration_iter}/{max_room_exploration_iter}\\n\")\n",
    "                # \t2.a.2 level_2_room_selection -> rX # Can be multiple times # max room exploration iter\n",
    "                # \t2.a.3 level_1_pose_selection -> pX # Only 1 time\n",
    "                r_room_to_explore, b_single_now_explore_plan = mind_palace_exploration.room_exploration.plan(belief_manager, T_episode_to_explore, robot_place)\n",
    "\n",
    "                text_image_insight = \"\"\n",
    "                num_explored_places = 0\n",
    "                \n",
    "                while num_explored_places < max_num_place_node_exploration:\n",
    "                    print(f\"\\n# Episode Exploration Iteration: {episode_exploration_iter}, Room Exploration Iteration: {room_exploration_iter}, Place Exploration Iteration: {num_explored_places}/{max_num_place_node_exploration}\\n\")\n",
    "                    print(\"Room to explore: \", r_room_to_explore)\n",
    "                    p_goal_poses = mind_palace_exploration.place_exploration.plan(belief_manager, T_episode_to_explore, r_room_to_explore) # Can be multiple poses, consider doing it one by one for exploration\n",
    "\n",
    "                    if len(p_goal_poses) == 0:\n",
    "                        print(\"No other place to explore from LLM answer. We will explore other room.\")\n",
    "                        break\n",
    "\n",
    "                    # 2.b (Exploration): Execute action “a” (explore(pX) or retrieve(TX, pX)), \n",
    "                    # get an image from the pose pX. in world model\n",
    "                    observed_images, image_paths = world_model.explore(T_episode_to_explore, p_goal_poses)\n",
    "                    if 'now' in T_episode_to_explore:\n",
    "                        robot_place, distance = world_model.move_robot(T_episode_to_explore, robot_place, p_goal_poses)\n",
    "                        stats_total_distance += distance\n",
    "                    stats_total_images_retrieved += len(image_paths) if \"now\" not in T_episode_to_explore else 0\n",
    "\n",
    "\n",
    "                    if len(image_paths) == 0:\n",
    "                        print(\"No image retrieved. We will explore other room.\")\n",
    "                        break\n",
    "\n",
    "\n",
    "                    # 2.c (VLM Inference): Given the image. In belief manager\n",
    "                    # 2.c.1 Query VLM: Do we see the target object y?\n",
    "                    # 2.c.2 Query VLM: If yes, also ask VLM to describe the image in relation to answering the question and \n",
    "                    bool_y_object_found, text_image_insight = mind_palace_exploration.vlm_image_analysis(image_paths, belief_manager)\n",
    "\n",
    "                    # If target object y found is yes:\n",
    "                    # 2.c.3 Add image and insight to the belief/memory.\n",
    "                    if bool_y_object_found:\n",
    "                        belief_manager.update_history(text_image_insight,\n",
    "                                                    observed_images, image_paths,\n",
    "                                                    T_episode_to_explore,\n",
    "                                                    r_room_to_explore,\n",
    "                                                    p_goal_poses) # todo add more room, location, time\n",
    "                        break\n",
    "                    \n",
    "                    belief_manager.update_place_exploration_memory(p_goal_poses)\n",
    "                    num_explored_places = len(belief_manager.H_a_place_exploration_action_history)\n",
    "\n",
    "                if bool_y_object_found:\n",
    "                    break\n",
    "                \n",
    "                belief_manager.update_room_exploration_memory(text_image_insight, T_episode_to_explore, r_room_to_explore)\n",
    "\n",
    "                room_exploration_iter += 1\n",
    "\n",
    "                if stats_total_images_retrieved > 100:\n",
    "                    print(\"Max number of images retrieved reached. We will stop the exploration.\")\n",
    "                    break\n",
    "\n",
    "            # To avoid break loop in memory consolidation we comment this\n",
    "            # if bool_y_object_found:\n",
    "            #     break\n",
    "            if bool_y_object_found is False:\n",
    "                belief_manager.S_EQA_reasoning_summary.append(\"Reasoning Iteration: \" + str(reasoning_iter) + \"The target object \" + str(belief_manager.y_object_to_search) + \" is not found in the episode: \" + str(T_episode_to_explore) + \"\\n\")\n",
    "            \n",
    "            episode_exploration_iter += 1\n",
    "\n",
    "        reasoning_iter += 1\n",
    "\n",
    "    # Answer regardless of the readiness\n",
    "    A_answer, reasoning_to_the_answer = eqa_reasoning.answer_the_question(belief_manager)\n",
    "    print(\"Answer: \", A_answer)\n",
    "    print(\"Reasoning: \", reasoning_to_the_answer)\n",
    "\n",
    "    # 4 Evaluation\n",
    "    eqa_evaluation = EQAEvaluation(question, robot_start_place, \n",
    "                                   GT_A_answer, GT_best_path)\n",
    "    # Evaluate the answer accuracy\n",
    "    answer_accuracy = eqa_evaluation.evaluate_answer_accuracy(A_answer)\n",
    "\n",
    "    # Evaluate the SPL\n",
    "    SPL = eqa_evaluation.evaluate_SPL(belief_manager, world_model, stats_total_distance) \n",
    "\n",
    "    # Evaluate the retrieval precision\n",
    "    number_retrieved_images = eqa_evaluation.count_retrieved_images(belief_manager)\n",
    "\n",
    "    print(\"Answer accuracy: \", answer_accuracy, \"(1-5)\")\n",
    "    print(\"SPL: \", SPL)\n",
    "    print(\"Number of retrieved images: \", number_retrieved_images)\n",
    "\n",
    "    result = {}\n",
    "    result[\"question_id\"] = question_id\n",
    "    result[\"question\"] = question\n",
    "    result[\"question_type\"] = question_type\n",
    "    result[\"GT_A_answer\"] = GT_A_answer\n",
    "    result[\"GT_best_path\"] = GT_best_path\n",
    "    result[\"A_answer\"] = A_answer\n",
    "    result[\"reasoning_to_the_answer\"] = reasoning_to_the_answer\n",
    "    result[\"answer_accuracy\"] = answer_accuracy\n",
    "    result[\"distance_traveled\"] = stats_total_distance\n",
    "    result[\"total_images_retrieved\"] = stats_total_images_retrieved\n",
    "    result[\"SPL\"] = SPL\n",
    "    result[\"reasoning_summary\"] = belief_manager.S_EQA_reasoning_summary\n",
    "\n",
    "    # Save the result to a line of an existing csv file\n",
    "    result_csv_file_path = os.path.join(\"../results\", result_name, \"results.csv\")\n",
    "    with open(result_csv_file_path, \"a\") as file:\n",
    "        file.write(f\"{question_id},{question_type},{question.replace(',', '')},{GT_A_answer.replace(',', '')},{str(GT_best_path).replace(',', '')},{A_answer.replace(',', '')},{answer_accuracy},{stats_total_distance},{stats_total_images_retrieved},{SPL}\\n\")\n",
    "\n",
    "    # Save the result in yaml file as well\n",
    "    result_file_path = os.path.join(\"../results/\" + result_name, question_id + \".yaml\")\n",
    "    with open(result_file_path, \"w\") as file:\n",
    "        yaml.dump(result, file, default_flow_style=False)\n",
    "\n",
    "    results[question_id] = result\n",
    "\n",
    "    prev_scene_name = scene_name\n",
    "\n",
    "# Name with the date, time\n",
    "now = datetime.datetime.now()\n",
    "result_file_path = os.path.join(\"../results/\" + result_name, \"1_results_\" + now.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".json\")\n",
    "with open(result_file_path, \"w\") as file:\n",
    "    json.dump(results, file)\n",
    "\n",
    "\n",
    "if b_save_the_logs:\n",
    "    # Restore original stdout\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "    # Close the file\n",
    "    f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openeqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
